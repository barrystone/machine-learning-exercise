{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "J89Uq_xd7G6y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# == Importing the dataset ==\n",
        "dataset = pd.read_csv('insurance.csv')\n",
        "x = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KqNAdok7wGBd"
      },
      "outputs": [],
      "source": [
        "# == Taking care of missing data ==\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
        "imputer.fit(x[:,0:1])\n",
        "x[:,0:1] = imputer.transform(x[:,0:1])\n",
        "imputer.fit(x[:,2:4])\n",
        "x[:,2:4] = imputer.transform(x[:,2:4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "z6bZCPd0wK6e"
      },
      "outputs": [],
      "source": [
        "# == Encoding categorical data ==\n",
        "## Encoding categorical data\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ct = ColumnTransformer(transformers = [('encoder',OneHotEncoder(),[1,4,5])],remainder = 'passthrough')\n",
        "x = np.array(ct.fit_transform(x))\n",
        "## Encoding the Dependent Variable\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8CaD3dR6wPxK"
      },
      "outputs": [],
      "source": [
        "# == Splitting the dataset into the Training set and Test set ==\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size =0.2, random_state =1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pRvRyAzLwTMU"
      },
      "outputs": [],
      "source": [
        "# == Feature Scaling (Standardisation method) ==\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "x_train[:,8:] = sc.fit_transform(x_train[:,8:])\n",
        "x_test[:,8:] = sc.transform(x_test[:,8:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lNQumS2wW3H",
        "outputId": "665ea449-2a09-4f0f-9b2c-f2422a8586b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " == x ==\n",
            "[[1.0 0.0 0.0 ... 19.0 27.9 0.0]\n",
            " [0.0 1.0 1.0 ... 18.0 33.77 1.0]\n",
            " [0.0 1.0 1.0 ... 28.0 33.0 3.0]\n",
            " ...\n",
            " [1.0 0.0 1.0 ... 18.0 36.85 0.0]\n",
            " [1.0 0.0 1.0 ... 21.0 25.8 0.0]\n",
            " [1.0 0.0 0.0 ... 61.0 29.07 0.0]]\n",
            "\n",
            " == y ==\n",
            "[1005   57  306 ...   32   91 1171]\n",
            "\n",
            "==============\n",
            "\n",
            " == x_train ==\n",
            "[[1.0 0.0 1.0 ... 1.0022862943641633 -0.6647447195470174\n",
            "  -0.9070577122378711]\n",
            " [0.0 1.0 1.0 ... 1.0022862943641633 -1.51402368505697\n",
            "  -0.07894188280568505]\n",
            " [0.0 1.0 1.0 ... -1.5042660745131606 1.0811768460878646\n",
            "  -0.9070577122378711]\n",
            " ...\n",
            " [1.0 0.0 0.0 ... 0.8590547304283163 0.7006345403882132 0.749173946626501]\n",
            " [1.0 0.0 0.0 ... 0.0712811287811573 -1.3800989251111697\n",
            "  0.749173946626501]\n",
            " [0.0 1.0 1.0 ... 1.2887494222358575 -0.44589206305022205\n",
            "  -0.07894188280568505]]\n",
            "\n",
            " == x_test ==\n",
            "[[0.0 1.0 1.0 ... -1.4326502925452371 0.7937285808383426\n",
            "  -0.9070577122378711]\n",
            " [0.0 1.0 1.0 ... 1.2887494222358575 0.14207029768743673\n",
            "  -0.9070577122378711]\n",
            " [0.0 1.0 1.0 ... 0.8590547304283163 1.0338132114728866\n",
            "  -0.9070577122378711]\n",
            " ...\n",
            " [0.0 1.0 0.0 ... 0.5725916025566221 1.3506595947592914 0.749173946626501]\n",
            " [1.0 0.0 1.0 ... -1.4326502925452371 0.9644009883302467\n",
            "  -0.9070577122378711]\n",
            " [1.0 0.0 1.0 ... -0.28679778105846043 -0.7423230865887921\n",
            "  -0.9070577122378711]]\n",
            "\n",
            " == y_train ==\n",
            "[ 730  712    6 ... 1298 1052  813]\n",
            "\n",
            " == y_test ==\n",
            "[  42  794  631  732   98 1242  665  817  195 1056  881  811  451  504\n",
            "    5  644 1097  460 1164  926  691 1265  571  602  116  479  622  591\n",
            "  386  241  819  822 1133 1190 1122  700 1220 1023  834 1252  293  825\n",
            " 1060  866  354 1178 1160 1199  688  963  875  836  133  650 1010  267\n",
            " 1266  884 1102    8  218  910 1031  162  888  629  726 1099  165  973\n",
            " 1334 1333   63  871  917 1268  175  182  774  739  198  863  764   86\n",
            " 1246 1276  501 1306  731  562 1068 1121  943  637   31  675  699  861\n",
            "  859  458  795  988 1029 1200 1153 1024   62  538  123  788  299  264\n",
            "   35  986  878   14  480  955 1053  128  640  753  425  508 1288  680\n",
            "  905 1050 1289  560 1217   71  507 1236 1129   61   36   34  710  565\n",
            "  950  420 1042  958  930  745   55  566   58  481  511  256 1187  835\n",
            "  564  422  709  708  674   70  244 1329  843  186  436   23 1240  736\n",
            "  775  397  369  233 1103 1142  217  469  138  974  245  415  887  931\n",
            "  646  553  239 1028  109  615  545  656  951 1313  210  164  496  659\n",
            "  522  103  809  585  447  534  848 1115  463   25  473  502   89  805\n",
            "  277 1111 1009  265  326 1314  756 1209    3 1244  142 1082  862  342\n",
            "   40   84  295  755 1007  942  237  948 1317  485  849  723 1013   41\n",
            "  596  273  576   94 1181   88 1077 1132 1037  613 1026  953  766  305\n",
            "  853  740  183  173 1198 1101   29 1322  583  497  586 1259 1284 1292\n",
            "  104  371]\n"
          ]
        }
      ],
      "source": [
        "## Showing results\n",
        "print(\"\\n == x ==\")\n",
        "print(x)\n",
        "print(\"\\n == y ==\")\n",
        "print(y)\n",
        "print(\"\\n==============\")\n",
        "\n",
        "print(\"\\n == x_train ==\")\n",
        "print(x_train)\n",
        "print(\"\\n == x_test ==\")\n",
        "print(x_test)\n",
        "print(\"\\n == y_train ==\")\n",
        "print(y_train)\n",
        "print(\"\\n == y_test ==\")\n",
        "print(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Predicting the Test set results (Multiple Linear Regression) ==\n",
            "[ 215.74418089  757.50021296  668.54471863  749.66205203  195.06300683\n",
            " 1128.62813924  689.32537242  796.1052557   324.51607307 1093.75791617\n",
            "  771.46711771  847.26011823  520.49137847  566.51283834  161.41485476\n",
            "  654.92776269  367.38715205  517.32140993  836.99958647  848.65234572\n",
            "  722.50924919 1133.07604217  614.20464544  609.92611397  243.9185895\n",
            "  538.65870559  654.13398494  632.22714857  480.28065149  366.20943929\n",
            "  801.03078707  435.61235763 1468.02694591  852.99666938 1341.93599587\n",
            "  668.21008624 1005.15995172  950.7173194   803.38375336 1293.23562824\n",
            "  396.03007337  784.23570523  727.20042386  816.16787092  411.6071233\n",
            "  820.45267733  224.13102026  949.30051411  646.78519149  930.00654835\n",
            "  781.44352242  790.17047396  239.09762752  655.87700444  879.92822066\n",
            "  443.01251978 1296.77112202  827.27562001  351.74566045  197.64758381\n",
            "  362.21483294  856.01409078  926.38150216  277.70601857  855.75636204\n",
            "  658.0080297   716.66279745  754.64376899  233.66154077  944.09772043\n",
            " 1437.73236687 1150.81640817  198.38750791  807.36601858  824.58208724\n",
            " 1279.52610972  269.64313123  295.7096442   727.090337    725.78577\n",
            "  261.45376993  830.17722017  734.26883436  239.12424627 1226.19857468\n",
            " 1022.92926704  550.88800181 1329.71735271  772.46254648  622.36929606\n",
            " 1106.20384428 1414.44168213  862.04578078  657.18162464  161.18394088\n",
            "  655.01643957  685.30077309  826.67954809  827.51478561  524.93797708\n",
            "  767.93084708  908.30092534 1047.92912111  841.34689002 1476.26164964\n",
            "  931.00563648  180.65120698  581.22762932  237.54528806  786.62042755\n",
            "  402.73448511  353.62316227  172.01575324  975.82153568  281.93620703\n",
            "  163.08976295  574.74440906  852.87412341  618.2856916   249.65271235\n",
            "  640.15634438  792.13691521  503.67686221  532.96693932 1420.56849763\n",
            "  701.84146849  856.56414371 1110.34216077 1285.45235294  625.26539053\n",
            "  829.70586989  159.75061611  587.49261153 1069.58825809  555.34846193\n",
            "  163.33719892  173.62046618  212.12628982  662.6888714   675.3238384\n",
            "  897.54500872  499.65189439 1069.75849516  878.57800394  898.3450016\n",
            "  711.65161489  169.64653282  626.21463227  194.26620768  522.64853522\n",
            "  588.54755947  358.18685931  802.92627042  827.05193764  626.57436374\n",
            "  493.59183093  733.39701937  714.14046407  702.8884747   245.9610379\n",
            "  364.26519055 1560.00220116  768.8008732   301.08028459  520.06886516\n",
            "  215.73477807 1111.36608288  709.30600741  735.1139017   498.49174739\n",
            "  452.48942703  379.28922109 1377.64213852 1440.55313048  308.42412293\n",
            "  555.48584438  278.93907868  945.05191397  336.76097983  542.59617355\n",
            "  824.86563792  841.9246835   667.53218048  673.66080404  313.03649085\n",
            "  923.91635906  227.7263091   697.75961733  546.45369271  693.56136766\n",
            "  878.70961517 1404.03943401  304.44119648  233.26036253  520.90037278\n",
            "  633.74184562  556.10783251  248.59262715  757.60976503  610.12949187\n",
            "  528.18884799  615.6181141   776.77640358  527.92373315  527.76857182\n",
            "  163.35146226  579.31780854  543.43203419  282.59414837  756.1484401\n",
            "  411.0932326  1343.69522791  430.90868724  382.86944246  440.84008331\n",
            " 1581.72572835  748.55666423  981.56476228  158.16319959 1248.62444329\n",
            "  275.3247711  1271.28097196  826.96779429  383.22679865  183.24874383\n",
            "  214.20896639  416.82735545  276.19126831  848.35101207  867.59403606\n",
            "  330.77346129  859.50173074 1393.98451047  520.98215255  827.10789547\n",
            "  697.36250504 1036.18690805  194.88291265  689.98466097  378.23569875\n",
            "  672.59690148  261.33646928  948.80833117  239.39623834  699.28143934\n",
            " 1426.09484973  591.83405782  652.68235077  319.07828483  887.28017119\n",
            "  787.62553966  416.52695577  742.12370235  730.67351823  324.25834433\n",
            "  343.63935035  876.40510821 1240.99400035  151.95684147 1584.59955622\n",
            "  617.76125387  544.53292962  632.89107806 1196.66065409 1284.64064823\n",
            " 1318.46113161  254.61030068  476.97157218]\n",
            "\n",
            " == Predicting single result (row-50) ==\n",
            "[781.44352242]\n",
            "row-50:  781.443522418941\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(x_train, y_train)\n",
        "\n",
        "y_pred = regressor.predict(x_test)\n",
        "print(\"== Predicting the Test set results (Multiple Linear Regression) ==\")\n",
        "print(y_pred)\n",
        "\n",
        "print(\"\\n == Predicting single result (row-50) ==\")\n",
        "print(regressor.predict([[0.0,1.0,1.0,0.0,0.0,0.0,1.0,0.0,1.5752125501075516,0.1469699840269174,-0.9070577122378711\n",
        "]]))\n",
        "print(\"row-50: \", y_pred[50])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "insurance-predict",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
